%\section{KL Divergence}
%\begin{align*}
%	\textrm{KL}(q(x)||p(x)) = \int q(x)\log \frac{q(x)}{p(x)} = \mathbb{E}_q \Bigg[\log \frac{q(x)}{p(x)}\Bigg]
%\end{align*}

\section{Latent Variable Modeling}

For each object $x_i$, we establish additional latent variable $z_i$ which denotes the index of gaussian from which $i$-th object was generated. Then our model is
$$p(X,Z|\theta) = \prod_{i=1}^{n}p(x_i,z_i|\theta) = \prod_{i=1}^{n}p(x_i|z_i,\theta)p(z_i|\theta) = \prod_{i=1}^{n}\mathcal{N}(x_i|\mu_{z_i},\sigma_{z_i}^2)\pi_{z_i},$$
where $\pi_{j} = p(z_i=j)$ are prior probability of $j$-th gaussian and $\theta = \{\mu_j, \sigma_j, \pi_j\}_{j=1}^K$. If we know both $X$ and $Z$ then we can obtain explicit ML-solution:
$$\theta_{ML} = \argmax_{\theta}p(X,Z|\theta) = \argmax_{\theta}\log p(X,Z|\theta).$$
However, in practice, we don't know $Z$, but only know $X$. Thus, we need to maximize w.r.t. $\theta$ the log of incomplete likelihood
\begin{align}
	\log p(X|\theta) & = \int q(Z)\log p(X|\theta)dZ\\
	& = \int q(Z)\log \frac{p(X,Z|\theta)}{p(Z|X,\theta)}dZ\\
	& = \int q(Z)\log \frac{q(Z)p(X,Z|\theta)}{q(Z)p(Z|X,\theta)}dZ\\
	& = \int q(Z)\log \frac{p(X,Z|\theta)}{q(Z)}dZ+ \int q(Z)\log \frac{q(Z)}{p(Z|X,\theta)}dZ\\
	& = \underbrace{\int q(Z)\log \frac{p(X,Z|\theta)}{q(Z)}dZ}_{\textrm{ELBO, } \mathcal{L}(q,\theta)}+ \textrm{KL}(q(Z)||\log p(Z|X,\theta))
	\label{eq:elbo}
\end{align}
The first line uses that $Z$ is not related to $X$, so the integration does not affect $X$. Note that $p(X|\theta)$ in Eq.\ref{eq:elbo} (1) does not depend on $Z$. Also note that $p$ does not depend of $q$, \textbf{so maximizing ELBO is equal to minimizing the KL divergence}. 

By using ELBO, we are able to maximize the incomplete likelihood. If you see the KL term, it is trying to minimize the divergence between $q(Z)$ and $p(Z)$ through maximizing ELBO.

\subsection{ELBO}

Note that $\textrm{KL}(q(Z)||\log p(Z|X,\theta))\geq 0$, thus $\mathcal{L}(q,\theta) \leq \log p(X|\theta)$. In other words, $\mathcal{L}(q,\theta)$ is a \textbf{lower bound} on $\log p(X|\theta)$.

Let's see ELBO in a different perspective
\begin{align*}
	\mathcal{L}(q,\theta) &=  \int q(Z)\log \frac{p(X,Z|\theta)}{q(Z)}dZ\\
	&= \int q(Z)\Big[\log p(X,Z|\theta)- \log q(Z)\Big]dZ\\
	&= \int q(Z)\Big[\log p(Z|X,\theta)+\log p(X|\theta)-\log q(Z)\Big]dZ\\
	&= \int q(Z)\log p(X|\theta)dZ+\int q(Z) \log\frac{ p(Z|X,\theta)}{ q(Z)}dZ\\
	&= \log p(X|\theta)-\textrm{KL}(q(Z)|p(Z|X,\theta))
\end{align*}
To maximize the above equation, we need to minimize KL divergence. 

%\section{Variational Lower Bound}
%Function $g(\xi, x)$ is called variational lower bound for function $f(x)$ iff
%\begin{itemize}
%	\item For all $\xi$ for all $x$ if follows $f(x)\geq g(\xi, x)$
%	\item For any $x_0$ there exists $\xi(x_0)$ such that $f(x_0)=g(\xi(x_0), x_0)$
%\end{itemize} 

\subsection{Expectation Maximization}
We want to maximize ELBO, $\mathcal{L}(q,\theta)$, to minimize KL divergence between $q(Z)$ and $\log p(Z|X,\theta)$.
$$\max_{q,\theta}\mathcal{L}(q,\theta) = \max_{q,\theta}\int q(Z)\log \frac{p(X,Z|\theta)}{q(Z)}dZ.$$
We start from initial point $\theta_0$ and iteratively repeat \Ni E-step and \Nii M-step, iteratively:
\begin{itemize}
	\item E-Step: $\theta_0$ is fixed. 
		$$q(Z) = \argmax_{q}\mathcal{L}(q,\theta) = \argmin_{q}\textrm{KL}(q(Z)|p(Z|X,\theta)) = p(Z|X,\theta_0).$$ 
		\begin{itemize}
			\item This is because, maximizing ELBO is equal to minimizing KL divergence and the minimum $q$ can be achieved when $q$ is equal to $p(Z|X,\theta_0)$.
			\item Now, we just have to evaluate $p(Z|X,\theta_0)$.
		\end{itemize}
	\item M-Step: $q$ is fixed.
		$$\theta_* = \argmax_{\theta}\mathcal{L}(q,\theta) = \argmax_{\theta}\mathbb{E}_{q(Z)}[\log p(X,Z|\theta)]$$
		\begin{itemize}
			\item Can be accomplished by taking derivatives
			\item Set $\theta_0=\theta_*$ and go to the E-Step until convergence
		\end{itemize}
	
\end{itemize}

\subsection{Categorical Latent Variables}
$z_i \in \{1,...,K\}$
$$p(x_i|\theta) = \sum_{k=1}^{K}p(x_i|k,\theta)p(z_i=k|\theta)$$
is simply a finite mixture of distributions. 

E-Step:
$$q(z_i=k) = p(z_i=k|x_i,\theta) = \frac{p(x_k|z_i=k,\theta)p(z_i=k|\theta)}{\sum_{l=1}^{K}p(x_i|z_i=l,\theta)p(z_i=l|\theta)}$$
M-Step:
$$\argmax_{\theta}\mathbb{E}_{q(Z)}[\log p(X,Z|\theta)] = \sum_{i=1}^{n}\mathbb{E}_{q(z_i)}[\log p(x_i,z_i|\theta)] = \sum_{i=1}^{n}\sum_{k=1}^{K}q(z_i=k)\log p(x_i,k|\theta)$$

For GMM, we model $p(x|z)$ as Gaussian.

%\subsection{Continous Latent Variables}
%Continuous latent variables can be regarded as a mixture model of continous distributions. 
%$$p(x_i|\theta) = \int p(z_i|x_i,\theta) dz_i = \int p(x_i|z_i,\theta)p(z_i|\theta) dz_i$$
%E-step can be done in a closed from only in case of conjugate distributions, otherwise the true posterior is intractadble.
%$$q(z_i) = p(z_i|x_i,\theta) = \frac{p(x_k|z_i,\theta)p(z_i|\theta)}{\int p(x_i|z_i,\theta)p(z_i|\theta)dz_i}$$
%
%Typically, continuous latent variables are used for dimension reduction techniques also known as \textbf{representation learning.}
